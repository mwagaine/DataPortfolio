{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "991be193-a850-4b81-9e12-0a4645b9c96f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setting-up the Glue job\n",
    "<ol>\n",
    "  <li>Log-in to AWS and head to the IAM Management Console to create a new role with a Glue use case, attaching the following policies:\n",
    "    <ul>\n",
    "      <li><i>S3FullAccess</i></li>\n",
    "      <li><i>RDSFullAccess</i></li>\n",
    "      <li><i>CloudWatchFullAccess</i></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Go to the AWS Glue Studio and create a new job, selecting the option <i>Spark script editor</i>.\n",
    "  <br>\n",
    "  In the <i>Job details</i> tab, do the following:\n",
    "    <ul>\n",
    "      <li>For <i>Name</i>, use the same JobName defined in the Lambda function script <a href='#'>here</a>.</li>\n",
    "      <li>For <i>IAM role</i>, select the role you have just created.</li>\n",
    "      <li>Change the <i>Requested number of workers</i> to 2.</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>In the <i>Script</i>, delete the default code present and replace it with the code in this notebook below, before clicking <i>Save</i>.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e92782f-bef4-4e69-9516-9e651a401c75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from awsglue.utils import getResolvedOptions\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Derive S3 location from which data will be extracted\n",
    "args = getResolvedOptions(sys.argv,['s3_target_path_key','s3_target_path_bucket'])\n",
    "bucket = args['s3_target_path_bucket']\n",
    "fileName = args['s3_target_path_key']\n",
    "print(bucket, fileName)\n",
    "inputFilePath = f\"s3a://{bucket}/{fileName}\"\n",
    "\n",
    "# Initialise Spark session to build dataframes where this data will be transformed\n",
    "spark = SparkSession.builder.appName(\"CDC\").getOrCreate()\n",
    "\n",
    "# Prepare schema for this data\n",
    "schema = StructType([\n",
    "                    StructField(\"ChartDate_WeekEnding\", DateType(), True),\n",
    "                    StructField(\"Song\", StringType(), True),\n",
    "                    StructField(\"Artists\", StringType(), True),\n",
    "                    StructField(\"Sales\", IntegerType(), True)])\n",
    "\n",
    "# Prepare credentials to load transformed data into the target database\n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "url = \"jdbc:mysql://{insert database endpoint url}/\"\n",
    "table = \"matt_schema.songsales\"\n",
    "user = \"{insert database username}\"\n",
    "password = \"{insert database password}\"\n",
    "\n",
    "# Read full load data into input dataframe\n",
    "if \"LOAD\" in fileName:\n",
    "    inputDF = spark.read.schema(schema).csv(inputFilePath)\n",
    "    inputDF = inputDF \\\n",
    "        .withColumnRenamed(\"_c0\",\"ChartDate_WeekEnding\") \\\n",
    "        .withColumnRenamed(\"_c1\",\"Song\") \\\n",
    "        .withColumnRenamed(\"_c2\",\"Artists\") \\\n",
    "        .withColumnRenamed(\"_c3\",\"Sales\")\n",
    "    \n",
    "    # Write original source data to target database\n",
    "    inputDF.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", driver) \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table) \\\n",
    "        .option(\"truncate\", \"true\") \\\n",
    "        .option(\"mode\", \"append\") \\\n",
    "        .option(\"user\",user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .save()\n",
    "\n",
    "# Now consider what happens after the source data has been transformed        \n",
    "else:\n",
    "    # Read CDC data into info dataframe\n",
    "    nu_schema = StructType([\n",
    "                    StructField(\"Action\", StringType(), True),\n",
    "                    StructField(\"ChartDate_WeekEnding\", DateType(), True),\n",
    "                    StructField(\"Song\", StringType(), True),\n",
    "                    StructField(\"Artists\", StringType(), True),\n",
    "                    StructField(\"Sales\", IntegerType(), True)])\n",
    "    infoDF = spark.read.schema(nu_schema).csv(inputFilePath)\n",
    "    infoDF = infoDF \\\n",
    "        .withColumnRenamed(\"_c0\",\"Action\") \\\n",
    "        .withColumnRenamed(\"_c1\",\"ChartDate_WeekEnding\") \\\n",
    "        .withColumnRenamed(\"_c2\",\"Song\") \\\n",
    "        .withColumnRenamed(\"_c3\",\"Artists\") \\\n",
    "        .withColumnRenamed(\"_c4\",\"Sales\")\n",
    "\n",
    "    # Read data from target database into output dataframe\n",
    "    outputDF = spark \\\n",
    "        .read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", driver) \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .load(schema=schema)\n",
    "    outputDF = outputDF \\\n",
    "        .withColumnRenamed(\"_c0\",\"ChartDate_WeekEnding\") \\\n",
    "        .withColumnRenamed(\"_c1\",\"Song\") \\\n",
    "        .withColumnRenamed(\"_c2\",\"Artists\") \\\n",
    "        .withColumnRenamed(\"_c3\",\"Sales\")\n",
    "    \n",
    "    # Transform output dataframe based on information inside info dataframe\n",
    "    for row in infoDF.collect():\n",
    "        # Update rows\n",
    "        if row['Action'] == 'U':\n",
    "            outputDF = outputDF \\\n",
    "                .withColumn(\"Artists\", \n",
    "                            when(outputDF[\"ChartDate_WeekEnding\"] == row[\"ChartDate_WeekEnding\"], \n",
    "                                 row[\"Artists\"]).otherwise(outputDF[\"Artists\"]))\n",
    "        # Insert rows\n",
    "        elif row['Action'] == 'I':\n",
    "            insertedRow = [list(row)[1:]]\n",
    "            newDF = spark.createDataFrame(insertedRow, schema)\n",
    "            outputDF = outputDF.union(newDF)\n",
    "        # Delete rows    \n",
    "        elif row['Action'] == 'D':\n",
    "            outputDF = outputDF \\\n",
    "                .filter(outputDF.ChartDate_WeekEnding != row['ChartDate_WeekEnding'])\n",
    "        \n",
    "    # Finally, overwrite data in the target database with the final output data\n",
    "    outputDF.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"truncate\", \"true\") \\\n",
    "    .option(\"user\",user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Glue job",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
